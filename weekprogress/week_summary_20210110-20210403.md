# Summary - 2021.1.10~2021.4.3

By LI Haoyang 2021.4.3

[TOC]

## Notes

- [对抗攻击——优化噪声的艺术](https://zhuanlan.zhihu.com/p/341533105)
- [Density&Robustness](https://zhuanlan.zhihu.com/p/355959295)
- [Transformer 的稳健性更好吗？](https://zhuanlan.zhihu.com/p/361105702)

- [Self-Attention and Convolution](../blogs/pages/Single-SelfAttentionAndConvolution.html)

## Paper Compleleted

- *Haoyang Li, Xinggang Wang. Noise Modulation: Let Your Model Interpret Itself. arXiv preprint , [arXiv:2103.10603](https://arxiv.org/abs/2103.10603)* (submitted to ICCV 2021, hope for acceptance)

    The attempt to replace adversarial training with noise modulation failed, but noise modulation does have some interesting effects on model. The motivation is quite intriguing, hope that reviewers like this style.

## Paper Reviewed (5)

- *Jean-Baptiste Cordonnier, Andreas Loukas & Martin Jaggi. On the Relationship between Self-attention and Convolution Layers.  ICLR 2020.* https://epfml.github.io/attention-cnn/

    ==They build a connection between self-attention and convolution layers, i.e. each convolution layer can be mapped into a self-attention layer. They also empirically discover that in shallow layers, self-attention layer behaves like convolution layer.==

- *Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein. Visualizing the Loss Landscape of Neural Nets. NPIS 2018.  NIPS 2018. **[ arXiv:1712.09913](https://arxiv.org/abs/1712.09913)***

    ==They invent a technique to visualize the landscape of loss function and found that skip connections make the loss landscape more smooth to optimize.==

- *Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, Xingjun Ma. Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets. ICLR 2020. **[ arXiv:2002.05990](https://arxiv.org/abs/2002.05990)***

    ==They discover that skip connections make the adversarial example more transferable.==

- *Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, Andreas Veit. Understanding Robustness of Transformers for Image Classification. **[ arXiv:2103.14586](https://arxiv.org/abs/2103.14586)***

    ==They gave an empirical study of the robustness of transformer (ViT), both on corrupted images and adversarial examples. The results show that transformer is not significantly more robust than convolutional neural networks.==

- *Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh. On the Adversarial Robustness of Visual Transformers. **[ arXiv:2103.15670](https://arxiv.org/abs/2103.15670)***

    ==They gave an empirical study on the adversarial robustness of transformer (ViT). Despite that they conclude transformer is more robust, their results show the same conclusion as that by Srinadh Bhojanapalli et al. The claim that transformer is more robust seems to be drawn based on a small threat model that has no practical meaning.==

## Experiments

- Exploration for the potential of noise modulation on adversarial robustness

    The best result using noise mixture modulation is ~26%, far below the baseline of robust accuracy.

- Experiments for the noise modulation written from the perspective of interpretability.


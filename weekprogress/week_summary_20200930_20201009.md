# Week summary - 2020.9.30~2020.10.09

By LI Haoyang

[TOC]

## Notes

<a href="../blogs/pages/Map-AdversarialExample.html">A Map of Adversarial Example (unfinished)</a>

<a href="../blogs/pages/Landscape-AdversarialExample.html">The Landscape of Adversarial Example(unfinished)</a>

<a href="../blogs/pages/Note-AdversarialExampleOD.html">Adversarial Example in Object Detection (unfinished)</a>

## Paper reviewed

### The Landscape of Adversarial Example

- [x] **Initial commit of adversarial example**: *C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2014.* ***[arXiv:1312.6199](https://arxiv.org/abs/1312.6199)***
- [x] **FGSM**: *Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations (ICLR), 2015.*
- [x] **DeepFool**: *Moosavidezfooli S, Fawzi A, Frossard P, et al. DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks. computer vision and pattern recognition, 2016: 2574-2582.* ***[arXiv:1511.04599](https://arxiv.org/abs/1511.04599)***
- [x] **Carlini and Wagner**: *Nicholas Carlini, David Wagner. Towards Evaluating the Robustness of Neural Networks. SP 2017. **[arXiv:1608.04644](https://arxiv.org/abs/1608.04644)***
- [x] **Papernot's substitute training**: *Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami. Practical Black-Box Attacks against Machine Learning. CCS 2017. **[arXiv:1602.02697](https://arxiv.org/abs/1602.02697)***
- [x] **Adversarial example in physical world**: *Alexey Kurakin, Ian Goodfellow, Samy Bengio. Adversarial example in the physical world. ICLR 2017. **[arXiv:1607.02533](https://arxiv.org/abs/1607.02533)***
- [x] **Universal adversarial perturbation**: *Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard. Universal adversarial perturbations. CVPR 2017. **[arXiv:1610.08401](https://arxiv.org/abs/1610.08401)***
- [x] **Formulated adversarial training**: *Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. ICLR 2018. **[arXiv:1706.06083](https://arxiv.org/abs/1706.06083)***
- [ ] **Adversarial training for free**: *Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S. Davis, Gavin Taylor, Tom Goldstein. Adversarial Training for Free! NIPS 2019 [**arXiv:1904.12843v2**](https://arxiv.org/abs/1904.12843v2)*
- [ ] **Adversarial examples are features**: *Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information Processing Systems, pages 125–136, 2019.* ***[arXiv:1905.02175v3](https://arxiv.org/abs/1905.02175v3)***
- [x] **Ensemble defense of adversarial attack**: *Huanrui Yang, Jingyang Zhang, Hongliang Dong, Nathan Inkawhich, Andrew Gardner, Andrew Touchet, Wesley Wilkes, Heath Berry, Hai Li. DVERGE: Diversifying Vulnerabilities for Enhanced Robust Generation of Ensembles. NIPS 2020. **[arXiv:2009.14720](https://arxiv.org/abs/2009.14720)***
- [x] **Adversarial Propagation**: *Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan Yuille, Quoc V. Le. Adversarial Examples Improve Image Recognition. CVPR 2020. **[arXiv:1911.09665v2](https://arxiv.org/abs/1911.09665v2)***

### Adversarial Example in Object Detection

- [x] **Adversarial glasses**: *Sharif, Mahmood & Bhagavatula, Sruti & Bauer, Lujo & Reiter, Michael. (2016). Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition. 1528-1540. 10.1145/2976749.2978392.*  *SIGSAC 2016*
- [x] **Dense Adversarial Generation**: *Cihang Xie1, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, Alan Yuille. Adversarial Examples for Semantic Segmentation and Object Detection. ICCV 2017. [**arXiv:1703.08603**](https://ui.adsabs.harvard.edu/link_gateway/2017arXiv170308603X/arXiv:1703.08603)*
- [x] **Robust Physical Attacks**: *Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, Dawn Song. Robust Physical-World Attacks on Deep Learning Models. CVPR 2018 **[ arXiv:1707.08945](https://arxiv.org/abs/1707.08945)***
- [x] **Adversarial patch**:*Thys S , Van Ranst W , Goedemé, Toon. Fooling automated surveillance cameras: adversarial patches to attack person detection. CVPR workshop 2019 [ **arXiv:1904.08653**](https://arxiv.org/abs/1904.08653)* 
- [x] **Adversarial T-shirt**: *Kaidi Xu, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, Mengshu Sun, Hongge Chen, Pin-Yu Chen, Yanzhi Wang, Xue Lin. Adversarial T-shirt! Evading Person Detectors in A Physical World. ECCV 2020. **[ arXiv:1910.11099](https://arxiv.org/abs/1910.11099)***
- [ ] **Invisibility cloak**: *Zuxuan Wu, Ser-Nam Lim, Larry Davis, Tom Goldstein. Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors. ECCV 2020. **[ arXiv:1910.14667](https://arxiv.org/abs/1910.14667)***
- [x] **Adversarial training for object detection**: *Haichao Zhang, Jianyu Wang. Towards Adversarially Robust Object Detection. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 421-430*
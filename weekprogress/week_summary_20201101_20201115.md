# Summary - 2020.11.01~2020.11.15

By LI Haoyang 2020.11.15

[TOC]

## Notes

- <a href="../blogs/pages/Note-AdversarialExplanation.html" target="_blank">Explanation of Robustness and Adversarial Example</a> `updated`

- <a href="../blogs/pages/Note-AdversarialInterpretation.html" target="_blank">Interprertation for Robustness and Adversarial Example</a> `updated`
- <a href="../blogs/pages/Note-AdversarialBenchmark.html" target="_blank">Benchmark Adversarial Defenses</a>
- <a href="../blogs/pages/Note-AdversarialDefenseBreach.html" target="_blank">Breach Adversarial Defenses</a>
- <a href="../blogs/pages/Note-AdversarialDefenseInference.html" target="_blank">Adversarial Defense at Inference</a>
- <a href="../blogs/pages/Note-AdversarialVerification.html" target="_blank">Provable Defense</a>
- <a href="../blogs/pages/Note-AdversarialDefenseStructure.html" target="_blank">Robust Structure</a>
- <a href="../blogs/pages/Note-AdversarialTraining.html" target="_blank">Adversarial Defense by Adversarial Training</a> `updated`
- <a href="../blogs/pages/Single-AdversarialPAC.html" target="_blank">PAC Learning with Adversary</a>

## Paper reviewed (23)

### Benchmark

- [x] *Dan Hendrycks, Thomas G. Dietterich. Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations. ICLR 2019. **[ arXiv:1807.01697](https://arxiv.org/abs/1807.01697)*** (**ImageNet-C and ImageNet-P**)
- [x] *Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Nicolas Flammarion, Mung Chiang, Prateek Mittal, Matthias Hein. RobustBench: a standardized adversarial robustness benchmark. ICLR 2020. **[ arXiv:2010.09670](https://arxiv.org/abs/2010.09670)*** (**Robustbench**)
- [x] *Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, Dawn Song. Natural Adversarial Examples. arXiv preprint 2020.**[ arXiv:1907.07174](https://arxiv.org/abs/1907.07174)*** (**ImageNet-A and ImageNet-O**)
- [x] *Francesco Croce, Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. ICML 2020. **[ arXiv:2003.01690](https://arxiv.org/abs/2003.01690)*** (**AutoAttack**)

### Breach

- [x] *Warren He, James Wei, Xinyun Chen, Nicholas Carlini, Dawn Song. Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong. arXiv preprint 2017. **[ arXiv:1706.04701](https://arxiv.org/abs/1706.04701)***
- [x] *Nicholas Carlini, David Wagner. Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods. AISec 2017. **[ arXiv:1705.07263](https://arxiv.org/abs/1705.07263)***

### Defense other than Adversarial Training

- [x] *Minghao Guo, Yuzhe Yang, Rui Xu, Ziwei Liu, Dahua Lin. When NAS Meets Robustness: In Search of Robust Architectures against Adversarial Attacks. CVPR 2020. [**arXiv:1911.10695**](https://arxiv.org/abs/1911.10695)*
- [x] *Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, Nate Kushman. PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples. ICLR 2018. **[ arXiv:1710.10766](https://arxiv.org/abs/1710.10766)***  (**PixelDefend**)
- [x] *Tianyu Pang, Kun Xu, Jun Zhu.  Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks. ICLR 2020. **Paper: https://openreview.net/forum?id=ByxtC2VtPB***  (**MI**)

### Advances along Adversarial Training

- [x] *Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He. Feature Denoising for Improving Adversarial Robustness. CVPR 2019.  [ **arXiv:1812.03411**](https://arxiv.org/abs/1812.03411)* (**Feature Denoising**)
- [x] *Maksym Andriushchenko, Nicolas Flammarion. Understanding and Improving Fast Adversarial Training. NIPS 2020.* ***Paper: https://infoscience.epfl.ch/record/278914*** (**AT with FGSM GradAlign**)
- [x] *Eric Wong, Leslie Rice, J. Zico Kolter. Fast is better than free: Revisiting adversarial training. ICLR 2020*. ***Paper: https://openreview.net/forum?id=BJx040EFvH&noteId=BJx040EFvH*** (**AT with FGSM RS**)
- [x] *Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, Mohan Kankanhalli. Attacks Which Do Not Kill Training Make Adversarial Learning Stronger. ICML 2020. [**arXiv:2002.11242**](https://arxiv.org/abs/2002.11242)* (**FAT**)
- [x] *Harini Kannan, Alexey Kurakin, Ian Goodfellow. Adversarial Logit Pairing. arXiv preprint 2018 **[ arXiv:1803.06373](https://arxiv.org/abs/1803.06373)*** (**ALP**)

### Verification or Provable Defense

- [x] *Jeet Mohapatra, Tsui-Wei (Lily)Weng, Pin-Yu Chen, Sijia Liu, Luca Daniel. Towards Verifying Robustness of Neural Networks Against A Family of Semantic Perturbations. CVPR 2020. **[ arXiv:1912.09533](https://arxiv.org/abs/1912.09533)*** (**Sematify-NN**)
- [x] *Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, Pengchuan Zhang. A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks. NIPS 2019. **[ arXiv:1902.08722](https://arxiv.org/abs/1902.08722)***
- [x] *Mislav Balunovic, Martin Vechev. Adversarial Training and Provable Defenses: Bridging the Gap. ICLR 2020.*  ***Paper: https://openreview.net/forum?id=SJxSDxrKDr*** (**COLT**)

### Interpretation of Robustness and Adversarial Example

- [x] *Tianyuan Zhang, Zhanxing Zhu. Interpreting Adversarially Trained Convolutional Neural Networks. ICML 2019. [ **arXiv:1905.09797**](https://arxiv.org/abs/1905.09797)*
- [x] *Shivam Garg, Vatsal Sharan, Brian Hu Zhang, Gregory Valiant. A Spectral View of Adversarially Robust Features. NIPS 2018. [ **arXiv:1811.06609**](https://arxiv.org/abs/1811.06609)* (*)
- [x] *Cihang Xie, Alan Yuille. Intriguing Properties of Adversarial Training at Scale. ICLR 2020.* ***Paper: https://openreview.net/forum?id=HyxJhCEFDS&noteId=rJxeamAAKB***
- [x] *Leslie Rice, Eric Wong, J. Zico Kolter. Overfitting in adversarially robust deep learning. arXiv preprint 2020 **[ arXiv:2002.11569](https://arxiv.org/abs/2002.11569)*** (**AT with PGD + Early Stop**)

### Theoretical Analysis

- [x] *Daniel Cullina, Arjun Nitin Bhagoji, Prateek Mittal. PAC-learning in the presence of evasion adversaries. NIPS 2018. [ **arXiv:1806.01471**](https://arxiv.org/abs/1806.01471)* (==SINGLE NOTE==)
- [x] *Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, Aleksander Madry. Adversarially Robust Generalization Requires More Data. NIPS 2018. [**arXiv:1804.11285**](https://arxiv.org/abs/1804.11285)*
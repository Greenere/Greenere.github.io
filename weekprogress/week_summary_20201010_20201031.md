# Summary - 2020.10.10~2020.10.31

By LI Haoyang 2020.11.1

[TOC]

## Notes

- <a href="https://greenere.github.io/blogs/pages/Note-FewShotLearningSurvey.html" target="_blank">Summarized Survey of Few-shot Learning</a>
- <a href="https://greenere.github.io/blogs/pages/Note-FewShotLearning.html" target="_blank">About Few-shot Learning</a>
- <a href="https://greenere.github.io/blogs/pages/Note-OrthogonalConvolution.html" target="_blank">Orthogonal Convolution</a>
- <a href="https://greenere.github.io/blogs/pages/Note-AdversarialExplanation.html" target="_blank">Explanation for Robustness and Adversarial Example</a>
- <a href="https://greenere.github.io/blogs/pages/Note-AdversarialDefense.html" target="_blank">Defenses against Adversarial Attacks</a>
- <a href="https://greenere.github.io/blogs/pages/Note-AdversarialTraining.html" target="_blank">Advances along Adversarial Training</a>

## Paper reviewed

### Few-shot Learning

- [x]  *Yaqing Wang, Quanming Yao. Few-shot Learning: A Survey. 2019. **[ arXiv:1904.05046](https://arxiv.org/abs/1904.05046)***
- [x] *Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra. Matching Networks for One Shot Learning. NIPS 2016.**http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning***
- [x] *Mengting Chen, Yuxin Fang, Xinggang Wang, Heng Luo, Yifeng Geng, Xinyu Zhang, Chang Huang, Wenyu Liu, Bo Wang. Diversity Transfer Network for Few-Shot Learning. AAAI 2020.**[ arXiv:1912.13182](https://arxiv.org/abs/1912.13182)***

### Orthogonal Convolution

- [x] *Jiayun Wang Yubei Chen Rudrasis Chakraborty Stella X. Yu. Orthogonal Convolutional Neural Networks. CVPR 2020*[ ***arXiv:1911.12207***](https://arxiv.org/abs/1911.12207)

### Adversarial Robustness and Explanation

- [x] *Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. 2016. Robustness of classifiers: from adversarial to random noise. In* *Proceedings of the 30th International Conference on Neural Information Processing Systems* *(**NIPS'16**). Curran Associates Inc., Red Hook, NY, USA, 1632–1640.* ***[ arXiv:1608.08967](https://arxiv.org/abs/1608.08967)***
- [x] *Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information Processing Systems, pages 125–136, 2019.* ***[arXiv:1905.02175v3](https://arxiv.org/abs/1905.02175v3)***
- [x] *Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin D. Cubuk, Justin Gilmer. A Fourier Perspective on Model Robustness in Computer Vision. NIPS 2019. **[ arXiv:1906.08988](https://arxiv.org/abs/1906.08988)***
- [x] *Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, Aleksander Madry. Robustness May Be at Odds with Accuracy. ICLR 2019. **[ arXiv:1805.12152](https://arxiv.org/abs/1805.12152)***
- [x] *Nic Ford, Justin Gilmer, Nicolas Carlini, Dogus Cubuk. Adversarial Examples Are a Natural Consequence of Test Error in Noise. [ **arXiv:1901.10513**](https://arxiv.org/abs/1901.10513)*

### Adversarial Defense

- [x] *Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy Dvijotham, Alhussein Fawzi, Soham De, Robert Stanforth, and Pushmeet Kohli. Adversarial robustness through local linearization. In NeurIPS, 2019. **[ arXiv:1907.02610](https://arxiv.org/abs/1907.02610)***
- [x] *Moustapha Cisse , Piotr Bojanowski, Edouard Grave, Yann Dauphin, Nicolas Usunier. Parseval Networks: Improving Robustness to Adversarial Examples. ICML 2017. **[ arXiv:1704.08847](https://arxiv.org/abs/1704.08847)***
- [x] *Anish Athalye, Nicholas Carlini, David Wagner. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. ICML 2018. **[ arXiv:1802.00420](http://export.arxiv.org/abs/1802.00420)***
- [x] *Alvin Chan, Yi Tay, Yew Soon Ong, Jie Fu. Jacobian Adversarially Regularized Networks for Robustness. ICLR 2020. [ **arXiv:1912.10185**](https://arxiv.org/abs/1912.10185)*
- [x] *Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, Alexey Kurakin. On Evaluating Adversarial Robustness. [**arXiv:1902.06705**](https://arxiv.org/abs/1902.06705)*
- [ ] *Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, Aleksander Madry. Adversarially Robust Generalization Requires More Data. NIPS 2018. [**arXiv:1804.11285**](https://arxiv.org/abs/1804.11285)*

### Advances along Adversarial Training

- [x] *Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, Quoc V. Le. Smooth Adversarial Training. **[ arXiv:2006.14536](https://arxiv.org/abs/2006.14536)***
- [x] *Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S. Davis, Gavin Taylor, Tom Goldstein. Adversarial Training for Free! [**arXiv:1904.12843v2**](https://arxiv.org/abs/1904.12843v2)*
- [x] *Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, Michael I. Jordan. Theoretically Principled Trade-off between Robustness and Accuracy. ICML 2019. **[ arXiv:1901.08573](https://arxiv.org/abs/1901.08573)***
- [x] *Jonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert Stanforth, Alhussein Fawzi, Pushmeet Kohli. Are Labels Required for Improving Adversarial Robustness?. NIPS 2019. [ **arXiv:1905.13725**](https://arxiv.org/abs/1905.13725)*
- [ ] *Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, Bin Dong. You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle. NIPS 2019.* 